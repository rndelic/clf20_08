{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Данные\n",
    "\n",
    "Данные в [архиве](https://drive.google.com/file/d/15o7fdxTgndoy6K-e7g8g1M2-bOOwqZPl/view?usp=sharing). В нём два файла:\n",
    "- `news_train.txt` тренировочное множество\n",
    "- `news_test.txt` тренировочное множество\n",
    "\n",
    "С некоторых новостных сайтов были загружены тексты новостей за период  несколько лет, причем каждая новость принаделжит к какой-то рубрике: `science`, `style`, `culture`, `life`, `economics`, `business`, `travel`, `forces`, `media`, `sport`.\n",
    "\n",
    "В каждой строке файла содержится метка рубрики, заголовок новостной статьи и сам текст статьи, например:\n",
    "\n",
    ">    **sport**&nbsp;&lt;tab&gt;&nbsp;**Сборная Канады по хоккею разгромила чехов**&nbsp;&lt;tab&gt;&nbsp;**Сборная Канады по хоккею крупно об...**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задача\n",
    "\n",
    "1. Обработать данные, получив для каждого текста набор токенов\n",
    "Обработать токены с помощью (один вариант из трех):\n",
    "    - pymorphy2\n",
    "    - русского [snowball стеммера](https://www.nltk.org/howto/stem.html)\n",
    "    - [SentencePiece](https://github.com/google/sentencepiece) или [Huggingface Tokenizers](https://github.com/huggingface/tokenizers)\n",
    "    \n",
    "    \n",
    "2. Обучить word embeddings (fastText, word2vec, gloVe) на тренировочных данных. Можно использовать [gensim](https://radimrehurek.com/gensim/models/word2vec.html) . Продемонстрировать семантические ассоциации. \n",
    "\n",
    "3. Реализовать алгоритм классификации, посчитать точноть на тестовых данных, подобрать гиперпараметры. Метод векторизации выбрать произвольно - можно использовать $tf-idf$ с понижением размерности (см. scikit-learn), можно использовать обученные на предыдущем шаге векторные представления, можно использовать [предобученные модели](https://rusvectores.org/ru/models/). Имейте ввиду, что простое \"усреднение\" токенов в тексте скорее всего не даст положительных результатов. Нужно реализовать два алгоритмов из трех:\n",
    "     - SVM\n",
    "     - наивный байесовский классификатор\n",
    "     - логистическая регрессия\n",
    "    \n",
    "\n",
    "4.* Реализуйте классификацию с помощью нейросетевых моделей. Например [RuBERT](http://docs.deeppavlov.ai/en/master/features/models/bert.html) или [ELMo](https://rusvectores.org/ru/models/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = list(open('./news_train.txt', 'r', encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "random.shuffle(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./news_test.txt', 'w', encoding='utf-8') as f:\n",
    "    for line in lines[12000:15000]:\n",
    "        f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'style': 242,\n",
       "         'forces': 987,\n",
       "         'media': 1655,\n",
       "         'science': 1737,\n",
       "         'sport': 1775,\n",
       "         'culture': 1642,\n",
       "         'life': 1612,\n",
       "         'travel': 225,\n",
       "         'economics': 1675,\n",
       "         'business': 450})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter([line.split('\\t')[0] for line in lines[:12000]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_train = lines[:12000]\n",
    "lines_test = lines[12000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_train = {'labels': [], 'headings': [], 'texts': []}\n",
    "dict_test = {'labels': [], 'headings': [], 'texts': []}\n",
    "\n",
    "for line in lines_train:\n",
    "    label, heading, text = line.split('\\t')\n",
    "    dict_train['labels'].append(label)\n",
    "    dict_train['headings'].append(re.findall(r'\\b\\w+\\b', heading.lower()))\n",
    "    dict_train['texts'].append(re.findall(r'\\b\\w+\\b', text.lower()))\n",
    "\n",
    "for line in lines_test:\n",
    "    label, heading, text = line.split('\\t')\n",
    "    dict_test['labels'].append(label)\n",
    "    dict_test['headings'].append(re.findall(r'\\b\\w+\\b', heading.lower()))\n",
    "    dict_test['texts'].append(re.findall(r'\\b\\w+\\b', text.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "dict_train['texts'] = [[morph.parse(word)[0].normal_form for word in text] for text in dict_train['texts']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_test['texts'] = [[morph.parse(word)[0].normal_form for word in text] for text in dict_test['texts']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=dict_train['texts'], workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('зимой', 0.7459695935249329),\n",
       " ('столетие', 0.7232862710952759),\n",
       " ('планетоид', 0.7136454582214355),\n",
       " ('опять', 0.7049204707145691),\n",
       " ('лето', 0.6933284997940063),\n",
       " ('переноситься', 0.6916874051094055),\n",
       " ('датироваться', 0.6900960206985474),\n",
       " ('весна', 0.6880436539649963),\n",
       " ('отремонтировать', 0.677342414855957),\n",
       " ('универсиада', 0.6722579002380371)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['зима'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('экспресс', 0.8114804029464722),\n",
       " ('р', 0.7826417684555054),\n",
       " ('мутко', 0.7095458507537842),\n",
       " ('футбол', 0.7056471109390259),\n",
       " ('виталий', 0.6898400783538818),\n",
       " ('хоккей', 0.6459627151489258),\n",
       " ('marca', 0.6164757013320923),\n",
       " ('тасс', 0.6100749969482422),\n",
       " ('исполком', 0.6044817566871643),\n",
       " ('федерация', 0.603024423122406)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['спорт'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [' '.join(text) for text in dict_train['texts']]\n",
    "y_train = dict_train['labels']\n",
    "X_test = [' '.join(text) for text in dict_test['texts']]\n",
    "y_test = dict_test['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "X_train_vec = tfidf.fit_transform(X_train)\n",
    "X_test_vec = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "RandCV_logreg = RandomizedSearchCV(LogisticRegression(), param_distributions={'C': np.arange(0.01, 1.01, 0.01)})\n",
    "RandCV_naivebayes = RandomizedSearchCV(MultinomialNB(), param_distributions={'alpha': np.arange(0.5, 1.51, 0.01)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv='warn', error_score='raise-deprecating',\n",
       "                   estimator=LogisticRegression(C=1.0, class_weight=None,\n",
       "                                                dual=False, fit_intercept=True,\n",
       "                                                intercept_scaling=1,\n",
       "                                                l1_ratio=None, max_iter=100,\n",
       "                                                multi_class='warn', n_jobs=None,\n",
       "                                                penalty='l2', random_state=None,\n",
       "                                                solver='warn', tol=0.0001,\n",
       "                                                verbose=0, warm_start=False),\n",
       "                   iid='warn', n_iter=10, n_jobs=None,\n",
       "                   param_distr...\n",
       "       0.45, 0.46, 0.47, 0.48, 0.49, 0.5 , 0.51, 0.52, 0.53, 0.54, 0.55,\n",
       "       0.56, 0.57, 0.58, 0.59, 0.6 , 0.61, 0.62, 0.63, 0.64, 0.65, 0.66,\n",
       "       0.67, 0.68, 0.69, 0.7 , 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77,\n",
       "       0.78, 0.79, 0.8 , 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88,\n",
       "       0.89, 0.9 , 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99,\n",
       "       1.  ])},\n",
       "                   pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "                   return_train_score=False, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RandCV_logreg.fit(X_train_vec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.8928819108748056\n",
      "Recall:  0.8673333333333333\n",
      "F1 score:  0.8759470587331706\n",
      "Accuracy:  0.8673333333333333\n"
     ]
    }
   ],
   "source": [
    "y_pred_lreg = RandCV_logreg.best_estimator_.predict(X_test_vec)\n",
    "print('Precision: ', precision_score(y_pred_lreg, y_test, average='weighted'))\n",
    "print('Recall: ', recall_score(y_pred_lreg, y_test, average='weighted'))\n",
    "print('F1 score: ', f1_score(y_pred_lreg, y_test, average='weighted'))\n",
    "print('Accuracy: ', accuracy_score(y_pred_lreg, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv='warn', error_score='raise-deprecating',\n",
       "                   estimator=MultinomialNB(alpha=1.0, class_prior=None,\n",
       "                                           fit_prior=True),\n",
       "                   iid='warn', n_iter=10, n_jobs=None,\n",
       "                   param_distributions={'alpha': array([0.5 , 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.6 ,\n",
       "       0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.7 , 0.71,\n",
       "       0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.8 , 0....\n",
       "       0.94, 0.95, 0.96, 0.97, 0.98, 0.99, 1.  , 1.01, 1.02, 1.03, 1.04,\n",
       "       1.05, 1.06, 1.07, 1.08, 1.09, 1.1 , 1.11, 1.12, 1.13, 1.14, 1.15,\n",
       "       1.16, 1.17, 1.18, 1.19, 1.2 , 1.21, 1.22, 1.23, 1.24, 1.25, 1.26,\n",
       "       1.27, 1.28, 1.29, 1.3 , 1.31, 1.32, 1.33, 1.34, 1.35, 1.36, 1.37,\n",
       "       1.38, 1.39, 1.4 , 1.41, 1.42, 1.43, 1.44, 1.45, 1.46, 1.47, 1.48,\n",
       "       1.49, 1.5 ])},\n",
       "                   pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "                   return_train_score=False, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RandCV_naivebayes.fit(X_train_vec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.8809479630495028\n",
      "Recall:  0.809\n",
      "F1 score:  0.8382154076508355\n",
      "Accuracy:  0.809\n"
     ]
    }
   ],
   "source": [
    "y_pred_nb = RandCV_naivebayes.predict(X_test_vec)\n",
    "print('Precision: ', precision_score(y_pred_nb, y_test, average='weighted'))\n",
    "print('Recall: ', recall_score(y_pred_nb, y_test, average='weighted'))\n",
    "print('F1 score: ', f1_score(y_pred_nb, y_test, average='weighted'))\n",
    "print('Accuracy: ', accuracy_score(y_pred_nb, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
